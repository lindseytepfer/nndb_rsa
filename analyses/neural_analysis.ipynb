{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29250f-1b08-43e7-9993-bc903cd27140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_path = \"/Volumes/Scraplab/NNDb_data/ds002837/derivatives/\"\n",
    "derivative_path = \"/Volumes/Scraplab/lindseytepfer/nndb/derivatives/\"\n",
    "\n",
    "sublist = [x for x in os.listdir(derivative_path) if ('sub-') in x] #grab all the subject IDs for easy filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74608d9",
   "metadata": {},
   "source": [
    "## <font color='deeppink'>Trim .nii Files</font>\n",
    "The NNDb data consists of 86 participants who watched 1 of 10 full-length films. In this study, we investigate 10 clips per movie, ranging from 15-25 seconds long. To analyze the brain data that corresponds with these clips, we trim the original .nii file into 10 shorter sub-files, according to when the participants would have watched each clip. Moreover, we shift the .nii trim timestamps by 4 seconds, to account for the hemodynamic response function in the brain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3bfd58-1d98-48c3-b646-1a30e5694415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of all 100 clips into a list\n",
    "\n",
    "clip_path = \"/Users/f004p74/Documents/dartmouth/projects/NNDb/movie_clips/\"\n",
    "clip_dir = os.listdir(clip_path)\n",
    "movie_list = [x for x in clip_dir if '.' not in x] #generates a list of all 10 movies\n",
    "clip_list = []\n",
    "\n",
    "zip_name, zip_start, zip_stop = [],[],[]\n",
    "\n",
    "for movie in movie_list:\n",
    "    clips = [x for x in os.listdir(clip_path+movie) if 'mp4' in x]\n",
    "    \n",
    "    for clip in clips:\n",
    "        name = clip.split('.')[0]\n",
    "        start = name.split('_')[1]\n",
    "        stop = name.split('_')[2]\n",
    "        clip_list.append(name)\n",
    "\n",
    "        zip_name.append(name)\n",
    "        zip_start.append(start)\n",
    "        zip_stop.append(stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36017c71-c780-4826-910c-599c41b2b802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_clip_dict = dict(zip(zip_name, zip(zip_start, zip_stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d79039-585b-451c-9b27-2bfc5aa5cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the participant .nii files\n",
    "\n",
    "server_path = \"/Volumes/Scraplab/NNDb_data/ds002837/derivatives/\"\n",
    "\n",
    "niilist = []\n",
    "\n",
    "for sub in sublist:\n",
    "    img = [x for x in os.listdir(server_path+sub+'/func/') if '_bold_blur_censor_ica.nii.gz' in x]\n",
    "    for x in img:\n",
    "        niilist.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in movie_list:\n",
    "    print(\"generating clips for: \", movie)\n",
    "\n",
    "    movie_clip_data = [x for x in clip_list if movie in x] #[split_5609_5633 ... split_2416_2437]\n",
    "    movie_neural_data = [x for x in niilist if movie in x] #[sub-71_task-split_bold_blur_censor_ica.nii.gz ...]\n",
    "\n",
    "    for sub in movie_neural_data:\n",
    "        sub_id = sub.split('_')[0]\n",
    "        sub_img = nb.load(server_path+sub_id+os.sep+'func'+os.sep+sub) #../derivatives/sub-71/func/sub-71_task-split_bold_blur_censor_ica.nii.gz\n",
    "\n",
    "        for clip in movie_clip_data: #each clip takes apprx 2m\n",
    "            start = int(clip_dict[clip][0]) + 4\n",
    "            stop = int(clip_dict[clip][1]) + 4\n",
    "            fname = sub_id+\"_\"+movie+\"_\"+str(start)+\"_\"+str(stop) #sub-71_split_5613_5637\n",
    "            print(fname)\n",
    "            \n",
    "            clip_slice = sub_img.slicer[:,:,:,start:stop]\n",
    "\n",
    "            nb.save(clip_slice, derivative_path+sub_id+os.sep+fname+\".nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804ee6e9-9beb-4476-a070-621a6da711e8",
   "metadata": {},
   "source": [
    "## <font color='deeppink'>Mask schaefer atlas to trimmed nii files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174103ba-5726-4339-a385-d36ba1918012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nb\n",
    "import nilearn\n",
    "from nilearn import datasets\n",
    "import nilearn.image as image\n",
    "from nilearn.maskers import NiftiMasker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12572c62-2440-4add-a5fb-bb6875f5d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=17, resolution_mm=1,\n",
    "                                                    data_dir=None, base_url=None, resume=True, verbose=1)\n",
    "'''\n",
    "From the documentation:\n",
    "The list of labels does not contain ‘Background’ by default. \n",
    "To have proper indexing, you should either manually add ‘Background’ to the list of labels:\n",
    "'''\n",
    "\n",
    "schaefer_atlas.labels = np.insert(schaefer_atlas.labels, 0, \"Background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a477b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# holding all 400 parcel masks in memory; takes apprx 2m13s\n",
    "mask_list = []\n",
    "\n",
    "for p in range(1,402): #402\n",
    "\n",
    "    try:\n",
    "        parcel = nilearn.image.new_img_like(schaefer_atlas.maps, nilearn.image.get_data(schaefer_atlas.maps) == p) #hold the parcel masks in memory \n",
    "        masker = NiftiMasker() \n",
    "\n",
    "        parcel_mask = masker.fit(parcel)\n",
    "        mask_list.append(parcel_mask)\n",
    "    \n",
    "    except: \n",
    "        print(\"out of range, p=\", p)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bbab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sub_list:\n",
    "    print(\"Starting with subject: \", s)\n",
    "    sub_clips = os.listdir(derivative_path+s)\n",
    "    sub_clips = [x for x in sub_clips if 'sub' in x]\n",
    "    sub_clips.sort() # super important, it has to be in ascending order. \n",
    "\n",
    "    for c in sub_clips[1:]: #2m 13s per clip\n",
    "        clip_slice = nb.load(derivative_path+s+os.sep+c)\n",
    "        clip_avg = image.mean_img(clip_slice)\n",
    "\n",
    "        clip_name = c.replace('.', '_')\n",
    "        clip_name = clip_name.split('_')[1:4]\n",
    "        clip_name = \"_\".join(clip_name)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for ix,mask in enumerate(mask_list):\n",
    "            try:\n",
    "                roi_data = mask.transform_single_imgs(clip_avg)\n",
    "                data_list.append(roi_data[0])\n",
    "            except:\n",
    "                print(\"index: \", ix)\n",
    "                continue\n",
    "\n",
    "        df = pd.DataFrame(data_list)\n",
    "        df.to_csv(derivative_path+s+os.sep+\"clip_voxels/\"+str(clip_name)+\".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47685a",
   "metadata": {},
   "source": [
    "# <font color='deeppink'>Creating Similartiy Matrices</font>\n",
    "<font color='deeppink'>Create a 10x10 similarity matrix: </font> for each parcel, I want to create a similarity matrix among all the 10 clips the participant watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ea873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sublist:\n",
    "    os.mkdir('/Volumes/Scraplab/lindseytepfer/nndb/derivatives/'+sub+'/parcel_correlations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c511e2-c627-40c0-844b-64a3e94348a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes too long to run locally\n",
    "for s in sublist:\n",
    "    spath = derivative_path+s+'/masked_nii/'\n",
    "    correlation_files = [x for x in  os.listdir(spath) if '.csv' in x]\n",
    "\n",
    "    df_list = [pd.read_csv(spath+x) for x in correlation_files]\n",
    "\n",
    "    for i in range(len(df_list[0])): \n",
    "        mat = pd.DataFrame()\n",
    "        \n",
    "        for ix, df in enumerate(df_list): # go into each df \n",
    "            voxels = df.iloc[i]\n",
    "            mat[correlation_files[int(ix)]] = voxels\n",
    "\n",
    "        mat.dropna(inplace=True)\n",
    "\n",
    "        #produces a 10x10 correlation matrix as a .csv for each parcel's voxel signal across the 10 movie clips\n",
    "        distance_matrix = pd.DataFrame(pairwise_distances(mat.T, metric='correlation'))\n",
    "        distance_matrix.to_csv(derivative_path+s+'/parcel_correlations/'+str(i)+\"_correlation.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138eb834",
   "metadata": {},
   "source": [
    "<font color='deeppink'>Import the 100x100 behavioral distance matrix.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "import statsmodels.api as sm\n",
    "from nilearn.connectome import ConnectivityMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_list = []\n",
    "\n",
    "for x in os.listdir(derivative_path+\"behavioral_matrices\"):\n",
    "    if '.csv' in x:\n",
    "        print(x)\n",
    "        df = pd.read_csv(derivative_path+\"behavioral_matrices/\"+x)\n",
    "        iv_list.append(df)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed0048",
   "metadata": {},
   "source": [
    "When we check whether there is co-linearity between the IV's, we find very high values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4827842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb057b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movie_tcv_ratings.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5381d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['watched', 'social', 'feeling', 'tension', 'conflict', 'violence', 'count_people', 'count_interactions']]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range (len(X.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073f31b",
   "metadata": {},
   "source": [
    "### <font color='hotpink'> Run simple regressions.</font> \n",
    " Next, we run simple regressions to get zero-order estimates for each of our independent variables (e.g., conflict, violence, tension). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa97b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes X minutes to run locally\n",
    "for s in sublist:\n",
    "    spath = derivative_path+s # ~/Volumes/Scraplab/S/lindseytepfer/nndb/derivatives/sub-s\n",
    "    smovielist = [x for x in os.listdir(spath) if 'sub' in x] #[sub-s_500daysofsummer_4841_4868.nii.gz, ....]\n",
    "    smovie = smovielist[0].split('_')[1] # 500daysofsummer\n",
    "\n",
    "    parcel_list = [x for x in  os.listdir(spath+'/parcel_correlations/') if 'correlation' in x] # 400 n_correlation.csv files \n",
    "    parcel_names = [x.split('_')[0] for x in parcel_list] # 0-399\n",
    "\n",
    "    results_df = pd.DataFrame(columns=[['conflict', 'watched', 'npeople', 'violence', 'ninteractions', 'feeling', 'tension', 'issocial']])\n",
    "\n",
    "    for p in parcel_list: #e.g., ['168_correlation.csv']\n",
    "        parcel_df = pd.read_csv(spath+'/parcel_correlations/'+p, header=None)\n",
    "        y = nilearn.connectome.sym_matrix_to_vec(np.array(parcel_df), discard_diagonal=True) # lower triangle, shape (45,)\n",
    "\n",
    "        all_iv = np.empty((45,8))\n",
    "        \n",
    "        for i in range(len(iv_list)): #iv_list = [conflict_distance_matrix.csv, violence_distance_matrix.csv ...]\n",
    "            df = iv_list[i]\n",
    "            filtered_df = df[(df.movie == smovie)]\n",
    "            col = [str(x) for x in list(filtered_df.index)]\n",
    "            \n",
    "            x_100 = filtered_df.drop(['movieID','movie'], axis=1)\n",
    "            x = x_100[col] #ensures corresponding movie columns\n",
    "            x_lower = nilearn.connectome.sym_matrix_to_vec(np.array(x), discard_diagonal=True) # lower triangle\n",
    "\n",
    "            all_iv[:,i] = x_lower\n",
    "        \n",
    "        iv_corrcoefs = []\n",
    "\n",
    "        for iv in range(8):\n",
    "            single_iv = all_iv[:,iv]\n",
    "            r = np.corrcoef(single_iv, y)[0][1]\n",
    "            iv_corrcoefs.append(r)\n",
    "\n",
    "        results_df.loc[len(results_df)] = iv_corrcoefs\n",
    "    \n",
    "    results_df['subject'] = s\n",
    "    results_df['movie'] = smovie\n",
    "    results_df['parcel'] = parcel_names\n",
    "\n",
    "    results_df.to_csv(spath+'/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(derivative_path+s+\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951f968",
   "metadata": {},
   "source": [
    "### <font color='deeppink'> Calculating Cohen's d</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ddbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, net each participant's model estimate \n",
    "# model_estimates = []\n",
    "\n",
    "# for s in sublist:\n",
    "#     model_estimates.append(pd.read_csv(derivative_path+s+'/results.csv'))\n",
    "\n",
    "# combined_estimates = pd.concat(model_estimates)\n",
    "# combined_estimates.reset_index(inplace=True, drop=True)\n",
    "# combined_estimates.drop(['Unnamed: 0'],inplace=True, axis=1)\n",
    "# combined_estimates.to_csv(derivative_path+\"/combined_estimates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eed891",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_estimates = pd.read_csv(derivative_path+'/combined_estimates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(columns=[['conflict', 'watched', 'npeople', 'violence', 'ninteractions', 'feeling', 'tension', 'issocial']])\n",
    "\n",
    "for movie in movie_list[0:1]:\n",
    "    movie_estimates = combined_estimates[(combined_estimates.movie == movie)]\n",
    "\n",
    "    for i in range(400):\n",
    "        parcel = movie_estimates.loc[movie_estimates.parcel == i ] #get the regression model estimates for a single parcel, within a movie\n",
    "\n",
    "        cols = ['conflict', 'watched', 'npeople', 'violence', 'ninteractions', 'feeling', 'tension', 'issocial']\n",
    "        dvals = []\n",
    "\n",
    "        for col in cols:\n",
    "            dvals.append(parcel[col].mean()/ parcel[col].std())\n",
    "\n",
    "        out.loc[len(out)] = dvals\n",
    "\n",
    "out.to_csv(derivatives+'/parcel_cohensd.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b107dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert betas to cohen's d value -- done\n",
    "\n",
    "out = pd.DataFrame(columns=[['conflict', 'watched', 'npeople', 'violence', 'ninteractions', 'feeling', 'tension', 'issocial']])\n",
    "\n",
    "for i in range(400):\n",
    "    parcel = combined_estimates.loc[combined_estimates.parcel == i]\n",
    "\n",
    "    cols = ['conflict', 'watched', 'npeople', 'violence', 'ninteractions', 'feeling', 'tension', 'issocial']\n",
    "    dvals = []\n",
    "    \n",
    "    for col in cols:\n",
    "        dvals.append(parcel[col].mean()/ parcel[col].std())\n",
    "\n",
    "    out.loc[len(out)] = dvals\n",
    "\n",
    "out.to_csv(derivatives+'/parcel_cohensd.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca27a96",
   "metadata": {},
   "source": [
    "### <font color='deeppink'> Permutation Analyses</font>\n",
    "\n",
    "Next, we will bootstrap the participants and permute the cohen's d values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "derivatives = \"/Users/f004p74/Documents/dartmouth/projects/NNDb/derivatives/\"\n",
    "\n",
    "sub_list = os.listdir(derivatives)\n",
    "sub_list = [x for x in sub_list if 'sub' in x] #remove hidden files\n",
    "\n",
    "results = pd.read_csv(derivatives+'/complete_betas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_mat = np.empty((400,10000,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4886e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['conflict', 'watched', 'npeople', 'violence', 'ninteractions', 'feeling', 'tension', 'issocial']\n",
    "\n",
    "for i in range(400): #for each parcel...\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    parcel = results.loc[results.parcel == i]\n",
    "\n",
    "    #bootstrap and permute the parcel dataframe\n",
    "    for p in range(10000):\n",
    "\n",
    "        sample_movies = parcel.sample(n=10, replace=True) #returns ... ?\n",
    "\n",
    "        participant_n = sample_movies['subject'].unique() # a list of all the participants\n",
    "\n",
    "        #set up the sign flipping at the participant level:\n",
    "        signs_arr = np.random.choice((-1,1), len(participant_n), replace=True) # returns an array of 1's and -1's of length 86\n",
    "        \n",
    "        for ix, subject in enumerate(participant_n): #create the sign-flipped dataframe\n",
    "\n",
    "            filtered = sample_movies[(sample_movies.subject == subject)]\n",
    "            filtered.loc[:, columns] = filtered[columns].apply(lambda x: x * signs_arr[ix], axis=1)\n",
    "            sample_movies[(sample_movies.movie == subject)] = filtered\n",
    "\n",
    "        #now with the bootstrap and the permutation orders set up, calculate new cohen d values:\n",
    "        perm_mat[i,p] = np.array(np.mean(sample_movies[columns],axis=0)/np.std(sample_movies[columns],axis=0))\n",
    "\n",
    "np.save(derivatives+'/permutation_matrix', perm_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd896c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['conflict', 'watched', 'npeople', 'violence', 'ninteractions', 'feeling', 'tension', 'issocial']\n",
    "\n",
    "for i in range(400): #for each parcel...\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    parcel = results.loc[results.parcel == i]\n",
    "\n",
    "    #bootstrap and permute the parcel dataframe\n",
    "    for p in range(10000):\n",
    "\n",
    "        boot_sample = parcel.sample(n=86, replace=True)\n",
    "        boot_movies = boot_sample['movie'].unique() # an array of movie names\n",
    "\n",
    "        #set up the sign flipping for each parcel sample's set of movies:\n",
    "        signs_arr = np.random.choice((-1,1), len(boot_movies), replace=True) # returns an array of 1's and -1's\n",
    "        \n",
    "        for ix, movie in enumerate(boot_movies): #create the sign-flipped dataframe\n",
    "\n",
    "            filtered = boot_sample[(boot_sample.movie == movie)]\n",
    "            filtered.loc[:, columns] = filtered[columns].apply(lambda x: x * signs_arr[ix], axis=1)\n",
    "            boot_sample[(boot_sample.movie == movie)] = filtered\n",
    "\n",
    "        #now with the bootstrap and the permutation orders set up, calculate new cohen d values:\n",
    "        perm_mat[i,p] = np.array(np.mean(boot_sample[columns],axis=0)/np.std(boot_sample[columns],axis=0))\n",
    "\n",
    "np.save(derivatives+'/permutation_matrix', perm_mat)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Bash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
