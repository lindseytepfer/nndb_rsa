{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29250f-1b08-43e7-9993-bc903cd27140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_path = \"/Volumes/Scraplab/NNDb_data/ds002837/derivatives/\"\n",
    "derivative_path = \"/Volumes/Scraplab/lindseytepfer/nndb/derivatives/\"\n",
    "\n",
    "sublist = [x for x in os.listdir(derivative_path) if ('sub-') in x] #grab all the subject IDs for easy filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74608d9",
   "metadata": {},
   "source": [
    "## <font color='deeppink'>Trim .nii Files</font>\n",
    "The NNDb data consists of 86 participants who watched 1 of 10 full-length films. In this study, we investigate 10 clips per movie, ranging from 15-25 seconds long. To analyze the brain data that corresponds with these clips, we trim the original .nii file into 10 shorter sub-files, according to when the participants would have watched each clip. Moreover, we shift the .nii trim timestamps by 4 seconds, to account for the hemodynamic response function in the brain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3bfd58-1d98-48c3-b646-1a30e5694415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of all 100 clips into a list\n",
    "\n",
    "clip_path = \"/Users/f004p74/Documents/dartmouth/projects/NNDb/movie_clips/\"\n",
    "clip_dir = os.listdir(clip_path)\n",
    "movie_list = [x for x in clip_dir if '.' not in x] #generates a list of all 10 movies\n",
    "clip_list = []\n",
    "\n",
    "zip_name, zip_start, zip_stop = [],[],[]\n",
    "\n",
    "for movie in movie_list:\n",
    "    clips = [x for x in os.listdir(clip_path+movie) if 'mp4' in x]\n",
    "    \n",
    "    for clip in clips:\n",
    "        name = clip.split('.')[0]\n",
    "        start = name.split('_')[1]\n",
    "        stop = name.split('_')[2]\n",
    "        clip_list.append(name)\n",
    "\n",
    "        zip_name.append(name)\n",
    "        zip_start.append(start)\n",
    "        zip_stop.append(stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36017c71-c780-4826-910c-599c41b2b802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_clip_dict = dict(zip(zip_name, zip(zip_start, zip_stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d79039-585b-451c-9b27-2bfc5aa5cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the participant .nii files\n",
    "\n",
    "server_path = \"/Volumes/Scraplab/NNDb_data/ds002837/derivatives/\"\n",
    "\n",
    "niilist = []\n",
    "\n",
    "for sub in sublist:\n",
    "    img = [x for x in os.listdir(server_path+sub+'/func/') if '_bold_blur_censor_ica.nii.gz' in x]\n",
    "    for x in img:\n",
    "        niilist.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in movie_list:\n",
    "    print(\"generating clips for: \", movie)\n",
    "\n",
    "    movie_clip_data = [x for x in clip_list if movie in x] #[split_5609_5633 ... split_2416_2437]\n",
    "    movie_neural_data = [x for x in niilist if movie in x] #[sub-71_task-split_bold_blur_censor_ica.nii.gz ...]\n",
    "\n",
    "    for sub in movie_neural_data:\n",
    "        sub_id = sub.split('_')[0]\n",
    "        sub_img = nb.load(server_path+sub_id+os.sep+'func'+os.sep+sub) #../derivatives/sub-71/func/sub-71_task-split_bold_blur_censor_ica.nii.gz\n",
    "\n",
    "        for clip in movie_clip_data: #each clip takes apprx 2m\n",
    "            start = int(clip_dict[clip][0]) + 4\n",
    "            stop = int(clip_dict[clip][1]) + 4\n",
    "            fname = sub_id+\"_\"+movie+\"_\"+str(start)+\"_\"+str(stop) #sub-71_split_5613_5637\n",
    "            print(fname)\n",
    "            \n",
    "            clip_slice = sub_img.slicer[:,:,:,start:stop]\n",
    "\n",
    "            nb.save(clip_slice, derivative_path+sub_id+os.sep+fname+\".nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804ee6e9-9beb-4476-a070-621a6da711e8",
   "metadata": {},
   "source": [
    "## <font color='deeppink'>Mask schaefer atlas to trimmed nii files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174103ba-5726-4339-a385-d36ba1918012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nb\n",
    "import nilearn\n",
    "from nilearn import datasets\n",
    "import nilearn.image as image\n",
    "from nilearn.maskers import NiftiMasker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12572c62-2440-4add-a5fb-bb6875f5d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=17, resolution_mm=1,\n",
    "                                                    data_dir=None, base_url=None, resume=True, verbose=1)\n",
    "'''\n",
    "From the documentation:\n",
    "The list of labels does not contain ‘Background’ by default. \n",
    "To have proper indexing, you should either manually add ‘Background’ to the list of labels:\n",
    "'''\n",
    "\n",
    "schaefer_atlas.labels = np.insert(schaefer_atlas.labels, 0, \"Background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a477b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# holding all 400 parcel masks in memory; takes apprx 2m13s\n",
    "mask_list = []\n",
    "\n",
    "for p in range(1,402): #402\n",
    "\n",
    "    try:\n",
    "        parcel = nilearn.image.new_img_like(schaefer_atlas.maps, nilearn.image.get_data(schaefer_atlas.maps) == p) #hold the parcel masks in memory \n",
    "        masker = NiftiMasker() \n",
    "\n",
    "        parcel_mask = masker.fit(parcel)\n",
    "        mask_list.append(parcel_mask)\n",
    "    \n",
    "    except: \n",
    "        print(\"out of range, p=\", p)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bbab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sub_list:\n",
    "    print(\"Starting with subject: \", s)\n",
    "    sub_clips = os.listdir(derivative_path+s)\n",
    "    sub_clips = [x for x in sub_clips if 'sub' in x]\n",
    "    sub_clips.sort() # super important, it has to be in ascending order. \n",
    "\n",
    "    for c in sub_clips[1:]: #2m 13s per clip\n",
    "        clip_slice = nb.load(derivative_path+s+os.sep+c)\n",
    "        clip_avg = image.mean_img(clip_slice)\n",
    "\n",
    "        clip_name = c.replace('.', '_')\n",
    "        clip_name = clip_name.split('_')[1:4]\n",
    "        clip_name = \"_\".join(clip_name)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for ix,mask in enumerate(mask_list):\n",
    "            try:\n",
    "                roi_data = mask.transform_single_imgs(clip_avg)\n",
    "                data_list.append(roi_data[0])\n",
    "            except:\n",
    "                print(\"index: \", ix)\n",
    "                continue\n",
    "\n",
    "        df = pd.DataFrame(data_list)\n",
    "        df.to_csv(derivative_path+s+os.sep+\"clip_voxels/\"+str(clip_name)+\".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47685a",
   "metadata": {},
   "source": [
    "# <font color='deeppink'>Creating Similartiy Matrices</font>\n",
    "<font color='deeppink'>Create a 10x10 similarity matrix: </font> for each parcel, I want to create a similarity matrix among all the 10 clips the participant watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ea873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sublist:\n",
    "    os.mkdir('/Volumes/Scraplab/lindseytepfer/nndb/derivatives/'+sub+'/parcel_correlations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c511e2-c627-40c0-844b-64a3e94348a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes too long to run locally\n",
    "for s in sublist:\n",
    "    spath = derivative_path+s+'/masked_nii/'\n",
    "    correlation_files = [x for x in  os.listdir(spath) if '.csv' in x]\n",
    "\n",
    "    df_list = [pd.read_csv(spath+x) for x in correlation_files]\n",
    "\n",
    "    for i in range(len(df_list[0])): \n",
    "        mat = pd.DataFrame()\n",
    "        \n",
    "        for ix, df in enumerate(df_list): # go into each df \n",
    "            voxels = df.iloc[i]\n",
    "            mat[correlation_files[int(ix)]] = voxels\n",
    "\n",
    "        mat.dropna(inplace=True)\n",
    "\n",
    "        #produces a 10x10 correlation matrix as a .csv for each parcel's voxel signal across the 10 movie clips\n",
    "        distance_matrix = pd.DataFrame(pairwise_distances(mat.T, metric='correlation'))\n",
    "        distance_matrix.to_csv(derivative_path+s+'/parcel_correlations/'+str(i)+\"_correlation.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138eb834",
   "metadata": {},
   "source": [
    "<font color='deeppink'>Import the 100x100 behavioral distance matrix.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "import statsmodels.api as sm\n",
    "from nilearn.connectome import ConnectivityMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_list = []\n",
    "\n",
    "for x in os.listdir(derivative_path+\"behavioral_matrices\"):\n",
    "    if '.csv' in x:\n",
    "        df = pd.read_csv(derivative_path+\"behavioral_matrices/\"+x)\n",
    "        iv_list.append(df)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed0048",
   "metadata": {},
   "source": [
    "When we check whether there is co-linearity between the IV's, we find very high values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4827842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb057b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movie_tcv_ratings.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5381d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['watched', 'social', 'feeling', 'tension', 'conflict', 'violence', 'count_people', 'count_interactions']]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range (len(X.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073f31b",
   "metadata": {},
   "source": [
    "### <font color='hotpink'> Run simple regressions.</font> \n",
    " Next, we run simple regressions to get zero-order estimates for each of our independent variables (e.g., conflict, violence, tension). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1134820",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(derivative_path+\"behavioral_matrices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa97b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 2 minutes/sub to run locally\n",
    "\n",
    "iv_filenames = os.listdir(derivative_path+\"behavioral_matrices\") #violence_distance_matrix.csv,\n",
    "iv_list = [] # a list of the CSV files\n",
    "\n",
    "for x in iv_filenames:\n",
    "    if '.csv' in x:\n",
    "        df = pd.read_csv(derivative_path+\"behavioral_matrices/\"+x)\n",
    "        iv_list.append(df)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "column_list = [x.split('_')[0] for x in iv_filenames] #violence, ..\n",
    "\n",
    "for s in sublist:\n",
    "    print('modeling:', s)\n",
    "    spath = derivative_path+s # ~/Volumes/Scraplab/S/lindseytepfer/nndb/derivatives/sub-s\n",
    "    smovielist = [x for x in os.listdir(spath) if 'sub' in x] #[sub-s_500daysofsummer_4841_4868.nii.gz, ....]\n",
    "    smovie = smovielist[0].split('_')[1] # 500daysofsummer\n",
    "\n",
    "    parcel_list = [x for x in  os.listdir(spath+'/parcel_correlations/') if 'correlation' in x] # 400 n_correlation.csv files \n",
    "    parcel_names = [x.split('_')[0] for x in parcel_list] # 0-399\n",
    "\n",
    "    results_df = pd.DataFrame(columns=[column_list])\n",
    "\n",
    "    for p in parcel_list: #e.g., ['168_correlation.csv']\n",
    "        parcel_df = pd.read_csv(spath+'/parcel_correlations/'+p, header=None)\n",
    "        y = nilearn.connectome.sym_matrix_to_vec(np.array(parcel_df), discard_diagonal=True) # lower triangle, shape (45,)\n",
    "\n",
    "        all_iv = np.empty((45,8))\n",
    "        \n",
    "        for i in range(len(iv_list)): #iv_list = [conflict_distance_matrix dataframe, violence_distance_matrix dataframe]\n",
    "            df = iv_list[i]\n",
    "            filtered_df = df[(df.movie == smovie)] #filter it based on what movie the subject watched\n",
    "            col = [str(x) for x in list(filtered_df.index)]\n",
    "            \n",
    "            x_100 = filtered_df.drop(['movieID','movie'], axis=1)\n",
    "            x = x_100[col] #ensures corresponding movie columns\n",
    "            x_lower = nilearn.connectome.sym_matrix_to_vec(np.array(x), discard_diagonal=True) # lower triangle\n",
    "\n",
    "            all_iv[:,i] = x_lower\n",
    "        \n",
    "        iv_corrcoefs = []\n",
    "\n",
    "        for iv in range(8):\n",
    "            single_iv = all_iv[:,iv]\n",
    "            r = np.corrcoef(single_iv, y)[0][1] #estimate the variable fit to the y (parcel matrix)\n",
    "            iv_corrcoefs.append(r)\n",
    "\n",
    "        results_df.loc[len(results_df)] = iv_corrcoefs\n",
    "    \n",
    "    results_df['subject'] = s\n",
    "    results_df['movie'] = smovie\n",
    "    results_df['parcel'] = parcel_names\n",
    "\n",
    "    results_df.to_csv(spath+'/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ddbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we net each subject's estimates into a single csv file\n",
    "model_estimates = []\n",
    "\n",
    "for s in sublist:\n",
    "    model_estimates.append(pd.read_csv(derivative_path+s+'/results.csv'))\n",
    "\n",
    "combined_estimates = pd.concat(model_estimates)\n",
    "\n",
    "combined_estimates.to_csv(derivative_path+\"/combined_estimates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951f968",
   "metadata": {},
   "source": [
    "### <font color='deeppink'> Calculating Cohen's d</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eed891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['violence', 'tension', 'social', 'watched', 'feeling', 'people', 'conflict', 'interactions']\n"
     ]
    }
   ],
   "source": [
    "combined_estimates = pd.read_csv(derivative_path+'/combined_estimates.csv')\n",
    "\n",
    "iv_filenames = os.listdir(derivative_path+\"behavioral_matrices\") #violence_distance_matrix.csv,\n",
    "iv_list = [] # a list of the CSV files\n",
    "\n",
    "for x in iv_filenames:\n",
    "    if '.csv' in x:\n",
    "        df = pd.read_csv(derivative_path+\"behavioral_matrices/\"+x)\n",
    "        iv_list.append(df)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "column_list = [x.split('_')[0] for x in iv_filenames] #violence, ..\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d03b30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_path = \"/Users/lindseytepfer/Dartmouth College Dropbox/Lindsey Tepfer/NNDb-rsa/nndb_movies/movie_clips\"\n",
    "clip_dir = os.listdir(clip_path)\n",
    "movie_list = [x for x in clip_dir if '.' not in x] #generates a list of all 10 movies\n",
    "\n",
    "dval_by_movie = np.empty([10,400,8])\n",
    "\n",
    "for ix, movie in enumerate(movie_list):\n",
    "    movie_estimates = combined_estimates[(combined_estimates.movie == movie)]\n",
    "\n",
    "    for p in range(400):\n",
    "        parcel = movie_estimates.loc[movie_estimates.parcel == p ] #get the regression model estimates for a single parcel, within a movie\n",
    "        dvals = []\n",
    "\n",
    "        for cx, col in enumerate(column_list):\n",
    "            if (movie == 'split') and (col == 'interactions'):\n",
    "                dvals.append(0)\n",
    "            else:\n",
    "                dvals.append(parcel[col].mean()/ parcel[col].std())\n",
    "        \n",
    "        dval_by_movie[ix,p,:] = dvals\n",
    "\n",
    "avg_movie_dval = dval_by_movie.mean(axis=0) #the actual statistics.\n",
    "\n",
    "np.save(derivative_path+'/cohens_dvals', avg_movie_dval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca27a96",
   "metadata": {},
   "source": [
    "### <font color='deeppink'> Permutation Analyses</font>\n",
    "\n",
    "Next, we will bootstrap the participants and permute the cohen's d values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "derivative_path = \"/Volumes/Scraplab/lindseytepfer/nndb/derivatives/\"\n",
    "sublist = [x for x in os.listdir(derivative_path) if ('sub-') in x] #grab all the subject IDs for easy filtering\n",
    "\n",
    "combined_estimates = pd.read_csv(derivative_path+'combined_estimates.csv')\n",
    "\n",
    "iv_filenames = os.listdir(derivative_path+\"behavioral_matrices\") #violence_distance_matrix.csv, ..\n",
    "column_list = [x.split('_')[0] for x in iv_filenames] #violence, ..\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f25c6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_mat = np.empty((400,10000,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4886e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(400)[0:1]: #for each parcel...\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    parcel = combined_estimates.loc[combined_estimates.parcel == i]\n",
    "\n",
    "    #bootstrap and permute the parcel dataframe\n",
    "    for p in range(10000)[0:1]:\n",
    "\n",
    "        #create a random sample (with replacement) from our list of 10 movies\n",
    "        sample_movies = np.random.choice(parcel.movie.unique(), 10)\n",
    "\n",
    "        parcel_list = []\n",
    "\n",
    "        for ix, mov in enumerate(sample_movies):\n",
    "            parcel_list.append(parcel[(parcel.movie == mov)])\n",
    "\n",
    "        full_sample = pd.concat(parcel_list).reset_index(drop=True)\n",
    "\n",
    "        #create a sign flipping array e.g., [-1, -1, 1, 1 ... 1]\n",
    "        signs_arr = np.random.choice((-1,1), len(full_sample), replace=True)\n",
    "\n",
    "        #Flip the signs depending on the signs_arr (sometimes no flip)\n",
    "        for ix in full_sample.index:\n",
    "            filtered = full_sample[(full_sample.index == ix)].copy()\n",
    "            print(\"test1\")\n",
    "            filtered.loc[:, column_list] = filtered[column_list].apply(lambda x: x * signs_arr[ix], axis=1)\n",
    "            print('test2')\n",
    "\n",
    "            #update the dataframe with the updated signs\n",
    "            full_sample[(full_sample.index == ix)] = filtered\n",
    "\n",
    "        #now with the bootstrap and the permutation orders set up, calculate new cohen d values:\n",
    "        perm_mat[i,p] = np.array(np.mean(full_sample[column_list],axis=0)/np.std(full_sample[column_list],axis=0))\n",
    "\n",
    "#save the numpy array for later processing\n",
    "#np.save(derivative_path+'/permutation_matrix', perm_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cbb83",
   "metadata": {},
   "source": [
    "### <font color='deeppink'> Use the permuted values to create a corrected null distribution</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b887244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['violence', 'tension', 'social', 'watched', 'feeling', 'people', 'conflict', 'interactions']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "derivative_path = \"/Volumes/Scraplab/lindseytepfer/nndb/derivatives/\"\n",
    "sublist = [x for x in os.listdir(derivative_path) if ('sub-') in x] #grab all the subject IDs for easy filtering\n",
    "\n",
    "iv_filenames = os.listdir(derivative_path+\"behavioral_matrices\") #violence_distance_matrix.csv, ..\n",
    "column_list = [x.split('_')[0] for x in iv_filenames] #violence, ..\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3db95cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_dvals = np.load(derivative_path+'/permutation_matrix.npy') #shape is (400,10000,8)\n",
    "cohens_dvals = np.load(derivative_path+'/cohens_dvals.npy') #shape is (400,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59af56e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the absolute value of the permuted p-values\n",
    "permuted_dvals = np.abs(permuted_dvals)\n",
    "\n",
    "#taking the maximum along the 400 axis gives us a null distribution for each IV\n",
    "null_dist = permuted_dvals.max(axis=0)\n",
    "\n",
    "results_pvals = np.empty((400,8))\n",
    "\n",
    "for iv in range(len(column_list)): #8, for each IV\n",
    "    for parcel in range(cohens_dvals.shape[0]): #400, for each parcel\n",
    "        iv_dvals = abs(cohens_dvals[parcel, iv])\n",
    "        results_pvals[parcel,iv] = np.mean(null_dist[:,iv] >= iv_dvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80863f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
